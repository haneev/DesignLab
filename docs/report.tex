\documentclass[12pt]{article}

\title{Report of FedWeb DesignLab Challenge}
\author{
        Han van der Veen \\
            \and
			Rik van Outersterp
}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
This is the paper's abstract
\end{abstract}

\section{Introduction}
This document elaborates our submission for the \textit{Design Challenge} assignment of the course \textit{Information Retrieval} ath the University of Twente. 
For this assignment a web result page for aggregated web search had to designed.
The results of this web search consist of a combination of multiple (independent) search engines.
The designed result page is planned to be used by the course' supervisors in a successive user experiment of which the purpose is to determine which pages give the best overall user experience, followed by a presentation of this study at the Text Retrieval Conference (TREC) in November 2014.

In this document we elaborate our web result pages.
The remainder of the document is setup as follows.
In section~\ref{sec:decisions} we .......
In sections~\ref{sec:layoutHan} and~\ref{sec:layoutRik} we present our two designs. 
A proposal for evaluating our pages is presented in section~\ref{sec:evaluation}.
This document is concluded in section~\ref{sec:conclusion}.

% Outline
I

\section{Decisions we made}
\label{sec:decisions}
- Added some features
- Why we have this two views

% Han's layout
\section{Design One}
\label{sec:layoutHan}


% Rik's layout
\section{Design Two}
\label{sec:layoutRik}
The second design has another approach than our first design.
In this design the results are ordered by type.
In order to do this a selection of the used search engines had to be made.
In the file FW14-engines.txt all used search engines were mentioned.
These search engines were grouped together when they are specialized in a similar area.
This aggregation led to twenty groups of types of websites.
In table~\ref{tab:selectionRik} in appendix~\ref{app:enginetypes} the selection is shown.

We decided to not develop a responsive layout due to time constraints.
Therefore it is important to note that this design was developed on a computer system with a resolution of 1680 x 1050 pixels.
When using this resolution or a higher resolution like 1920 x 1080 pixels the display of the design should be correct.
When considering the screen width, all resolutions from 1024 x 768 pixels and higher should be able to display five categories beside each other, although the text might be not userfriendly to read at the lower resolutions.
When considering the screen height, all resolutions from 1280 x 1024 pixels and higher should be able to display the headings of the second row of categories.


\section{How to evaluate this project designs}
\label{sec:evaluation}
To test this new layouts we have to establish at least two layouts to compare. In order to get a good comparison we need to control the variables of the design. The user has a question in mind, and which design will answer that question the best. 

To perform this evaluation we need to have a control, which can be in our case the 10 results for each search engine in a tabbed environment. The user can find the information by clicking the right tab and will find the information in that tab. The other designs we have are blended designs.

We will test two things. Are the results the correct results and how fast will the user find his answer. 

To find out if a design performs better than the baseline we will measure the time the user will have its answer. The faster the user finds its answer the better. We will measure this by the time the user enters the query until clicking on the answer page. In order to perform this evaluation several test subjects are needed that will perform several queries on the different designs. They never will have to do the same query in at two designs, because that will give them some notion of what they are looking for. So, every person has to perform 30 queries. 10 on the base design, 10 on version 1 and 10 on version 2. The time is compared for the same query for different persons.

To automate this process our blended designs are evaluated with a list of answers pages the experts described by~\cite{lalmas2011aggregated} have given for each query. Our automated blended design must have those answers as a result, or else our design is wrong. The more results pages we have the experts have given the better our design is performing. This can be automatic by counting the results which are in the result or not. 

When both tests result in that some design is better than the other we will say that the design is better. % wut :P

\section{Conclusions}
\label{sec:conclusion}
Small recap of what we did

\section{Checklist}
\begin{enumerate}
\item Check if our readme is valid with a new clean install
\end{enumerate}

\bibliographystyle{abbrv}
\bibliography{libfile}

\appendix
\section{Search Engine Types}
\label{app:enginetypes}
\include{enginetypes}

\end{document}
